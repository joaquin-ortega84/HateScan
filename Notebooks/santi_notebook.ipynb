{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('../data/df_sample25.csv')\n",
    "new_row = pd.DataFrame({\n",
    "    'TweetID': [1344796664837637121],\n",
    "    'LangID': [1],\n",
    "    'TopicID': [1],\n",
    "    'HateLabel': [2],\n",
    "    'TweetText': ['I hate all people in this word!, I am really angry kill kill kill, death!']\n",
    "})\n",
    "df = pd.concat([df, new_row], ignore_index=True)\n",
    "df['TweetText'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.replace('â€™', '')\n",
    "    sentence = sentence.lower() ## lowercase\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## stay with letter\n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "\n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenize\n",
    "    stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "\n",
    "    tokenized_sentence_cleaned = [ w for w in tokenized_sentence if not w in stop_words] ## remove stopwords\n",
    "    lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"v\") for word in tokenized_sentence_cleaned]\n",
    "    noun_lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"n\") for word in lemmatized]\n",
    "    cleaned_text = ' '.join(word for word in noun_lemmatized)\n",
    "    return cleaned_text\n",
    "\n",
    "def tokenizer(df):\n",
    "    X_train = [text_to_word_sequence(_) for _ in df[\"Cleaned_text\"]]\n",
    "    return X_train\n",
    "\n",
    "def vectorizer(X_train):\n",
    "    word2vec = Word2Vec(sentences=X_train, vector_size=100, window=3)\n",
    "    return word2vec\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "\n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Cleaned_text'] = df['TweetText'].apply(preprocessing)\n",
    "\n",
    "#train X\n",
    "X_train = tokenizer(df)\n",
    "\n",
    "#vectorize words in sentence\n",
    "word2vec = vectorizer(X_train)\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
