{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Generating New Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "# #openai.api_key = # REPLACE WITH YOUR API KEY\n",
    "# def get_hate(text):\n",
    "\n",
    "#     response = openai.Completion.create(\n",
    "#     model=\"text-davinci-002\",\n",
    "#     prompt=f\"{text}\",\n",
    "#     )\n",
    "\n",
    "#     return response['choices'][0]['text']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# worst_tweets = df.loc[df.HateLabel == 2]\n",
    "\n",
    "# def process_row(row):\n",
    "#     old_hate = row['TweetText']\n",
    "#     new_hate = get_hate(f'Give me similar texts to this one but use other hate words: \"{old_hate}\"')\n",
    "\n",
    "#     # Create new row\n",
    "#     new_row = {'TweetID': 'NewTweetID',  # Add appropriate TweetID\n",
    "#                'LangID': row['LangID'],\n",
    "#                'TopicID': row['TopicID'],\n",
    "#                'HateLabel': row['HateLabel'],\n",
    "#                'TweetText': new_hate} \n",
    "\n",
    "#     return new_row\n",
    "\n",
    "# worst_tweets = df.loc[df.HateLabel == 2]\n",
    "\n",
    "# for each in range(5):\n",
    "#     with Pool(cpu_count()) as p:\n",
    "#         new_rows = p.map(process_row, worst_tweets.to_dict('records'))\n",
    "\n",
    "#     new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "#     worst_tweets = pd.concat([worst_tweets, new_df], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_tweets = pd.read_csv('worst_tweets.csv')\n",
    "df = pd.read_csv('twitter_data.csv').dropna()\n",
    "\n",
    "# Concatenate the two datasets\n",
    "df = pd.concat([df, worst_tweets.iloc[:10000, :]], ignore_index=True).drop('TweetID', axis=1).drop_duplicates().dropna()\n",
    "baseline = df.HateLabel.value_counts().sort_values().values[-1] / df.HateLabel.value_counts().sum()\n",
    "print(f'Baseline for our model: {np.round(baseline,2)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hate_scan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
